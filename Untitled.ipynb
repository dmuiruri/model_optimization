{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa166501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "assert float(tf.__version__[:3]) >= 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ebd67b",
   "metadata": {},
   "source": [
    "## Generate a Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65de046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9ddfcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.5207 - accuracy: 0.8566 - val_loss: 0.1349 - val_accuracy: 0.9633\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.1259 - accuracy: 0.9644 - val_loss: 0.0877 - val_accuracy: 0.9748\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0820 - accuracy: 0.9760 - val_loss: 0.0723 - val_accuracy: 0.9769\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0695 - accuracy: 0.9802 - val_loss: 0.0633 - val_accuracy: 0.9801\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0596 - accuracy: 0.9816 - val_loss: 0.0583 - val_accuracy: 0.9807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa736229c10>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1\n",
    "train_images = train_images.astype(np.float32) / 255.0\n",
    "test_images = test_images.astype(np.float32) / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(28,28)),\n",
    "    tf.keras.layers.Reshape(target_shape=(28,28, 1)),\n",
    "    tf.keras.layers.Conv2D(filters=12, kernel_size=(3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bbffee",
   "metadata": {},
   "source": [
    "## Convert to a TensorFlow Lite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03f1cb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpbn09k4ew/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpbn09k4ew/assets\n",
      "2021-12-09 18:10:05.160276: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-12-09 18:10:05.161050: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2021-12-09 18:10:05.163751: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.006ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2021-12-09 18:10:05.210384: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\n",
      "2021-12-09 18:10:05.210453: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60ca9b9",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e089c0e9",
   "metadata": {},
   "source": [
    "### Convert using float fallback quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b7ae35d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpvbsqruw3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpvbsqruw3/assets\n",
      "2021-12-09 18:10:57.410095: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-12-09 18:10:57.410759: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2021-12-09 18:10:57.413181: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.004ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "\n",
      "2021-12-09 18:10:57.454372: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\n",
      "2021-12-09 18:10:57.454437: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\n",
      "2021-12-09 18:10:57.474041: I tensorflow/lite/tools/optimize/quantize_weights.cc:222] Skipping quantization of tensor sequential_1/conv2d_1/Conv2D because it has fewer than 1024 elements (108).\n"
     ]
    }
   ],
   "source": [
    "# Only quantizes weights by default\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model_quant_ff_weights = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8747485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 12)        120       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2028)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                20290     \n",
      "=================================================================\n",
      "Total params: 20,410\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dabc7a",
   "metadata": {},
   "source": [
    "### Convert using float fallback quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "298ba4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpe7dmo1lu/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpe7dmo1lu/assets\n",
      "2021-12-09 18:11:18.109287: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-12-09 18:11:18.109698: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2021-12-09 18:11:18.112145: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.007ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2021-12-09 18:11:18.153403: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\n",
      "2021-12-09 18:11:18.153471: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\n"
     ]
    }
   ],
   "source": [
    "# If ops does not support quantized operations, fallback to float\n",
    "# Quantize weights and variable data too\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_model_quant_ff_all = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "45e2ea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: <class 'numpy.float32'>\n",
      "output: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant_ff_weights)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print(f'input: {input_type}')\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print(f'output: {output_type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f427f0",
   "metadata": {},
   "source": [
    "### Convert using integer-only quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9c3ba749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnxm5jkez/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnxm5jkez/assets\n",
      "2021-12-09 18:11:43.352401: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-12-09 18:11:43.353033: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2021-12-09 18:11:43.355383: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2021-12-09 18:11:43.398952: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\n",
      "2021-12-09 18:11:43.399021: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\n"
     ]
    }
   ],
   "source": [
    "# Had to downgrade python-flatbuffers for this to run\n",
    "# !conda list | grep flat\n",
    "# Quantize weights and variable data too but using uint8\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant_uint8 = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "262d470d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant_uint8)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "96fcc8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24648"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# save the unquantized/float model\n",
    "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)\n",
    "\n",
    "# save the quantized model float fall back\n",
    "tflite_model_quant_file = tflite_models_dir/'mnist_model_quant_float_ff_weights_only.tflite'\n",
    "tflite_model_quant_file.write_bytes(tflite_model_quant_ff_weights)\n",
    "\n",
    "# save the quantized model float fall back\n",
    "tflite_model_quant_file = tflite_models_dir/'mnist_model_quant_float_ff_weights_data.tflite'\n",
    "tflite_model_quant_file.write_bytes(tflite_model_quant_ff_all)\n",
    "\n",
    "# save the quantized model\n",
    "tflite_model_quant_file = tflite_models_dir/'mnist_model_quant_uint8.tflite'\n",
    "tflite_model_quant_file.write_bytes(tflite_model_quant_uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fda85d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 516\r\n",
      "drwxrwxr-x  4 ubuntu   4096 Dec  9 17:48 .\r\n",
      "-rw-rw-r--  1 ubuntu  24672 Dec  9 17:48 mnist_model_quant_float_ff_weights_only.tflite\r\n",
      "-rw-rw-r--  1 ubuntu  23888 Dec  9 17:48 mnist_model_quant.tflite\r\n",
      "-rw-rw-r--  1 ubuntu  84488 Dec  9 17:48 mnist_model.tflite\r\n",
      "drwxrwxrwt 14 root     4096 Dec  9 17:45 ..\r\n",
      "-rw-rw-r--  1 ubuntu 271400 Dec  9 17:30 mnist_orig.h5\r\n",
      "-rw-rw-r--  1 ubuntu  97901 Dec  9 17:26 saved_model.pb\r\n",
      "drwxr-xr-x  2 ubuntu   4096 Dec  9 17:26 assets\r\n",
      "drwxr-xr-x  2 ubuntu   4096 Dec  9 17:26 variables\r\n"
     ]
    }
   ],
   "source": [
    "!ls -altG /tmp/mnist_tflite_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a2479e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does converting a model to tensorflowlite reduce its size before quantization ?\n",
    "# https://www.tensorflow.org/lite/performance/model_optimization#:~:text=with%20TensorFlow%20Lite.-,Quantization,model%20size%20and%20faster%20computation.\n",
    "model.save(\"/tmp/mnist_tflite_models/mnist_orig.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9f25f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 488\r\n",
      "-rw-rw-r--  1 ubuntu 271400 Dec  9 17:30 mnist_orig.h5\r\n",
      "drwxrwxr-x  4 ubuntu   4096 Dec  9 17:30 .\r\n",
      "-rw-rw-r--  1 ubuntu  97901 Dec  9 17:26 saved_model.pb\r\n",
      "drwxr-xr-x  2 ubuntu   4096 Dec  9 17:26 assets\r\n",
      "drwxr-xr-x  2 ubuntu   4096 Dec  9 17:26 variables\r\n",
      "-rw-rw-r--  1 ubuntu  24576 Dec  9 17:19 mnist_model_quant.tflite\r\n",
      "-rw-rw-r--  1 ubuntu  84488 Dec  9 17:19 mnist_model.tflite\r\n",
      "drwxrwxrwt 14 root     4096 Dec  9 17:15 ..\r\n"
     ]
    }
   ],
   "source": [
    "!ls -altG  /tmp/mnist_tflite_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd6bfe61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 564\r\n",
      "drwxrwxr-x  4 ubuntu   4096 Dec  9 17:57 .\r\n",
      "-rw-rw-r--  1 ubuntu  24672 Dec  9 17:57 mnist_model_quant_float_ff_weights_data.tflite\r\n",
      "-rw-rw-r--  1 ubuntu  23888 Dec  9 17:57 mnist_model_quant_float_ff_weights_only.tflite\r\n",
      "-rw-rw-r--  1 ubuntu  24576 Dec  9 17:57 mnist_model_quant_uint8.tflite\r\n",
      "-rw-rw-r--  1 ubuntu  84488 Dec  9 17:57 mnist_model.tflite\r\n",
      "drwxrwxrwt 14 root     4096 Dec  9 17:54 ..\r\n",
      "-rw-rw-r--  1 ubuntu  23888 Dec  9 17:48 mnist_model_quant.tflite\r\n",
      "-rw-rw-r--  1 ubuntu 271400 Dec  9 17:30 mnist_orig.h5\r\n",
      "-rw-rw-r--  1 ubuntu  97901 Dec  9 17:26 saved_model.pb\r\n",
      "drwxr-xr-x  2 ubuntu   4096 Dec  9 17:26 assets\r\n",
      "drwxr-xr-x  2 ubuntu   4096 Dec  9 17:26 variables\r\n"
     ]
    }
   ],
   "source": [
    "!ls -altG /tmp/mnist_tflite_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "05a23b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /tmp/mnist_tflite_models/mnist_model_quant*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a2f4ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /tmp/mnist_tflite_models/mnist_model.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a95f50f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 544\r\n",
      "drwxrwxr-x  4 ubuntu   4096 Dec  9 18:12 .\r\n",
      "-rw-rw-r--  1 ubuntu  24752 Dec  9 18:12 mnist_model_quant_float_ff_weights_data.tflite\r\n",
      "-rw-rw-r--  1 ubuntu  23968 Dec  9 18:12 mnist_model_quant_float_ff_weights_only.tflite\r\n",
      "-rw-rw-r--  1 ubuntu  24648 Dec  9 18:12 mnist_model_quant_uint8.tflite\r\n",
      "-rw-rw-r--  1 ubuntu  84564 Dec  9 18:12 mnist_model.tflite\r\n",
      "drwxrwxrwt 14 root     4096 Dec  9 18:11 ..\r\n",
      "-rw-rw-r--  1 ubuntu 271400 Dec  9 17:30 mnist_orig.h5\r\n",
      "-rw-rw-r--  1 ubuntu  97901 Dec  9 17:26 saved_model.pb\r\n",
      "drwxr-xr-x  2 ubuntu   4096 Dec  9 17:26 assets\r\n",
      "drwxr-xr-x  2 ubuntu   4096 Dec  9 17:26 variables\r\n"
     ]
    }
   ],
   "source": [
    "!ls -altG /tmp/mnist_tflite_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "37fbe9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ebd3802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model.pb</th>\n",
       "      <th>model.h5</th>\n",
       "      <th>weights_f32</th>\n",
       "      <th>weights_data_f32</th>\n",
       "      <th>weights_data_uint8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97901</td>\n",
       "      <td>271400</td>\n",
       "      <td>23968</td>\n",
       "      <td>24752</td>\n",
       "      <td>24648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model.pb  model.h5  weights_f32  weights_data_f32  weights_data_uint8\n",
       "0     97901    271400        23968             24752               24648"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'model.pb': [97901], 'model.h5': [271400], 'weights_f32':[23968], 'weights_data_f32': [24752], 'weights_data_uint8': [24648]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab0c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
